{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdSUUz8mpE/qmuOmsew/9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izzyolejnik/Machine-Learning/blob/master/HW5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F33Wvwtn3wap",
        "colab_type": "text"
      },
      "source": [
        "# **HW5**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAmV13OL34Bd",
        "colab_type": "text"
      },
      "source": [
        "# **General Concepts - (Artificial Intelligence, Machine Learning, Deep Learning)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fTFCyVl4DRz",
        "colab_type": "text"
      },
      "source": [
        "Artifical intelligence, machine learning, and deep learning are like a set up russian dolls nested within each other.\n",
        "\n",
        "AI is an umbrella term that describes any time a computer is doing something smart. Machine learning falls under AI and deep learning fall machine learning.\n",
        "\n",
        "**Artificial intelligence** emcompasses the idea that a computer can do intelligent things like a human, this includes: visual perception, speech recognition, decision-making, etc. \n",
        "\n",
        "**Supervised, Unsupervised, Reinforcement Learning**\n",
        "\n",
        "Machine Learning is the process of training a model to make useful predictions using a data set.\n",
        "\n",
        "**Supervised** refers to when a model is provided with training data that is labeled. \n",
        "\n",
        "Data has components known as: **features** and **labels**.\n",
        "\n",
        "**Features** are measurements or descriptions.\n",
        "\n",
        "**Labels** are the answer.\n",
        "\n",
        "**Training** is the process of feeding features and labels into an algorithm to determine the model.\n",
        "\n",
        "**Model** is the relationship between the features and labels.\n",
        "\n",
        "**Unsupervised** refers to when the machine must learn on it#s own without labels.\n",
        "\n",
        "**Reinforced learning** is when the machine interacts with a data and learns from a trial and error process.#\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm1dCldWLiS6",
        "colab_type": "code",
        "outputId": "98b595da-25bd-4466-d689-d42adad4bf37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Here is an example of training data from our MNIST digit homework.\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images_one, train_labels_one), (test_images_one, test_labels_one) = mnist.load_data()\n",
        "\n",
        "train_images = train_images_one.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "\n",
        "test_images = test_images_one.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvEMAKDA4EUT",
        "colab_type": "text"
      },
      "source": [
        "# **Basic Concepts - (Linear Regression, Logistic Regression, Gradients, Gradient Descent)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCktFeel4NuH",
        "colab_type": "text"
      },
      "source": [
        "**Regression** is a type of model that predicts continious values.\n",
        "\n",
        "In contrast, a **classification** model predicts discrete values.\n",
        "\n",
        "**Linear regression** is a particular type of regression model. This model places a linear relationship between data points that is known as the line of best fit. \n",
        "\n",
        "**Steps:**\n",
        "> *1. Plot known data*\n",
        "\n",
        "> *2. Approximate the relationship, a line is in the form $y = wx+b$*\n",
        "\n",
        "> *3. Make predictions with test and training data*\n",
        "\n",
        "In the formula $y=wx+b$, *y* is the predicted label, *b* is the bias, *w* is the weight, and *x* is the feature.\n",
        "\n",
        "**Logistic Regression** builds off the linear model by choosing a value between 0 and 1 through the sigmoid function. Because logistic regression uses only the values 0 and 1, it uses binary classifcation. The closer the ouput is to 0 or 1 when given input, the higher the correlation. \n",
        "\n",
        "**The Sigmoid function** is $y = \\frac{1}{1+e^{-z}}$\n",
        "\n",
        "When making a prediction, there can be loss based on the accuracy of the prediction when compared to the actual point.\n",
        "The goal of training is find values for weights and baises that lead to a low loss.\n",
        "\n",
        "**Gradient** is a vector that gives the direction in which the steepest ascent. The gradient descent is essentially a derivative. \n",
        "\n",
        "**Gradient Descent** is an iterative approach at finding a local minimum of a function. This semester, we looked at stochastic gradient descent and mini-batch gradient descent in our homework two. In machine learning, gradient descent is used in order to update the parameters of a model in a way that leads to minimal loss. Mini batch gradient descent randomly chooses the size of a batch per iteration. A **batch** is the amount of samples gone through iteration. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4x7kMWYMEF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An example code snippet from HW3 PT 2. Using Keras to implement logistic regression\n",
        "# This will not run because it is only a fragment.\n",
        "\n",
        "# Create the model with all the correct layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, \n",
        "          activation='sigmoid', \n",
        "          input_shape=(2,)))\n",
        "model.compile(optimizer='sgd', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "# Train model and then evaulate it\n",
        "history = model.fit(data_train, label_train, epochs=250, validation_data = (data_test, label_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJfoFpo7UMoS",
        "colab_type": "text"
      },
      "source": [
        "Below is an example code snippet from HW2 Part 2 displaying mini batch gradient descent. As you can see, the gradient is recorded each iteration in order to help minimize the loss. This functions also shows the use of lr, or a learning rate, which is also essential for minimizing loss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydyFKJ6aUIL7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mini(x, y, epochs, m, lr, weights):\n",
        "  batch = 4\n",
        "  weight_path = []\n",
        "  for epoch in range(epochs):\n",
        "    indices = np.random.permutation(m)\n",
        "    x_shuff = x[indices]\n",
        "    y_shuff = y[indices]\n",
        "    for i in range(0,m,batch):\n",
        "      x_i = x_shuff[i:i+batch]\n",
        "      y_i = y_shuff[i:i+batch]\n",
        "      gradient = 1 / batch * x_i.T.dot(x_i.dot(weights) - y_i)\n",
        "      weights = weights - lr * gradient\n",
        "      weight_path.append(weights)\n",
        "  return weight_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY9r0kON4OS-",
        "colab_type": "text"
      },
      "source": [
        "# **Building a Model - (Structure and Components of a covnet)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p577V5nc4U2t",
        "colab_type": "text"
      },
      "source": [
        "In order to build models, this semester we have utilized Keras in google colab. **Keras** is a deep learning framework that enables the user with easy access to data sets along with a convenient way to train and test almost any learning model.\n",
        "\n",
        "The typical keras work flow is:\n",
        "> Define training data\n",
        "\n",
        "> Define a model, or, network of layers\n",
        "\n",
        "> Choose a loss function and optimizer\n",
        "\n",
        "> Iterate on data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXYg0TkDFHMY",
        "colab_type": "code",
        "outputId": "fbce5358-bde1-4520-91d2-9612587c03d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# Code snippet illustrating dense layers for a model: the building blocks\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        " \n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, input_shape=(2,)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wf8Azu8Ir62",
        "colab_type": "text"
      },
      "source": [
        "**Layers** are the building blocks of a model. \n",
        "\n",
        "\n",
        "Examples of **activation layers** are *sigmoid*, *relu*, or *softmax*.\n",
        "\n",
        "A **CNN** or **Convolution Neural Network** is a network containing at least one convolutional layer. Convolution Neural Networks are made up convolution layers, poolling layers, and classification. It analyzes data while using supervised learning. A CNN typically is used in image classification. CNN's typically are able to extract more features by convolving on an area. Essentially, convolution uses a filter to convolve over the matrices. Once the convolution is complete, a resuting matrix is created known as the feature map. Our final HW, homework 4, is exemplary of a 2D convolution and max pooling function. Pooling is done in order to maximize the value of the information within the feature map. Max pooling in particular, does this by choosing the largest value within a certain size. Finally, softmax is the activation function used in order to create a fully interconnected layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83_d7C-m4e9l",
        "colab_type": "text"
      },
      "source": [
        "## **Compiling a Model - (Optimizer and Learning Rate)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_zJ63zT4g-v",
        "colab_type": "text"
      },
      "source": [
        "An **optimizer** is what helps the model continiously update itself. The optimizer is based on the loss function. \n",
        "\n",
        "The **loss function** you choose will be based off the problem you are solving. Some examples of loss functions include, *binary_crossentropy*, *categorical_crossentropy*, or *mse*. \n",
        "\n",
        "The **learning rate** is a tuning parameter. The LR helps to minimize the loss by determining the step size of each iteration within an optimization algorithm. We used learning rate in a few of our homeworks in order to improve the process of training. For example, in HW 3 PT 2 and 3. \n",
        "\n",
        "Compiling a model is essential for training, because training utilizes both the optimizer and the loss function. \n",
        "\n",
        "Within the compile function, the loss, optimizer, and metrics are set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHyaimcnWDf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model with all the correct layers\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(1, \n",
        "          activation='sigmoid', \n",
        "          input_shape=(2,)))\n",
        "model.compile(optimizer='sgd', \n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZCd_Jo2WNa0",
        "colab_type": "text"
      },
      "source": [
        "Above is a sample code snippet from HW3 that shows a model being built for logistic regression (hence the binary crossentropy loss) as well as the stochastic gradient descent optimizer, and accuracy metric. This model was used to train for logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urxJE5Xm4hUO",
        "colab_type": "text"
      },
      "source": [
        "## **Training a Model - (Underfitting / Overfitting)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qATOKw434qH9",
        "colab_type": "text"
      },
      "source": [
        "When training a model, data is usually split into two sets. The train data and the test data. Typically in our assignments, the split was done 80% and 20% respectively. \n",
        "\n",
        "The **test set** should be representative of the data as a whole and large enough to be yeild meaningful results.\n",
        "\n",
        "The **training set** is the data that is used to train the model. The model uses the training set and extracts the statistical patterns to build the model. The goal of training a model is to find the parameters that lead to the most optimal result\n",
        "\n",
        "Once the training is completed, the test set should be ran through the model to see its accuracy.\n",
        "\n",
        "However, this is when the concepts of overfitting and underfitting come to play.\n",
        "\n",
        "**Underfitting** occurs when the data hasn't been properly generalized and therefore performs poorly on the training set. In order to fix this, create a more complex model, or, create a wider variety of data.\n",
        "\n",
        "on the contrary...\n",
        "\n",
        "**Overfitting** occurs when the model is too complex. This leads to a flat accuracy level. This can lead to an issue when ran on test data. In order to fix this, create a less complex model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvcRaJhKeIn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training set\n",
        "trainx1 = x1[:80]\n",
        "trainx2 = x2[:80]\n",
        "trainy  =  y[:80]\n",
        "\n",
        "# test set\n",
        "testx1 = x1[80:]\n",
        "testx2 = x2[80:]\n",
        "testy  =  y[80:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-0FD95Neb4U",
        "colab_type": "text"
      },
      "source": [
        "Above is an example code snippet from homework 2 pt 2 of splitting the data into a test set and train set of the portions 80/20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcIzSU1_4qfh",
        "colab_type": "text"
      },
      "source": [
        "## **Finetuning a Pretrained Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT8o4Qre4v9P",
        "colab_type": "text"
      },
      "source": [
        "Fine tuning a pretrained model is the idea of using a pre-existing model that has been tested on wider data and using it on your smaller data while also tuning some of the parameters to fit your needs. For example, a user could add their own layers to the pre-existing model and freeze the pretrained model to only train the new layers. By doing so, the pretrained model is preserved while also considering the effects of the newly trained layers.\n",
        "\n",
        "Finetuning was practiced in our HW4 pt 3 section where we edited Professor Wocjans notebook to use other convolutional bases.\n",
        "\n",
        "Below is a code snippet from my HW4 pt 3 that shows finetuning of a pretrained densenet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww5ugZQvcVlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import densenet\n",
        "\n",
        "## Here is the convolutional base\n",
        "conv_base = densenet.DenseNet121(input_shape=(224, 224, 3),\n",
        "                                 weights='imagenet', \n",
        "                                 include_top=False)\n",
        "\n",
        "## This freezes the pretrained model\n",
        " conv_base.trainable = False\n",
        "\n",
        "## This adds the layer to the convolutional base\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "## Quite a bit is skipped for simplicity\n",
        "## ...\n",
        "\n",
        "## Below shows how up to the last layer the convolution base is frozen in order\n",
        "## to allow training for the new layer\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv5_block1_0_bn':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}